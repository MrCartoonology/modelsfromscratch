drop_into_debugger_on_error: True
tokenizer: "bert-base-uncased"
device: "cpu"

dataloader:
#  data_dir: "/Users/davidschneider/data/language/books/project_gutenberg"
  # get a shallow copy of the pytorch repo - git clone --depth 1 https://github.com/pytorch/pytorch
  files:
    data_dir: "/Users/davidschneider/code/pytorch/pytorch"
    include_hidden_dirs: False
    include_hidden_files: True
    only_include_specified_extensions: True  
    specified_extensions:
      - ".py"
  max_mb: 100
  train_ratio: 0.8
  seq_len: 128
  batch_size: 64
  shuffle: True

model_name: "transformer"
models:
  transformer:
    model_dim: 64
    depth: 6
    num_heads: 4
    ff_hidden_dim: 2048
    freq_base: 10000
  
  basic_rnn:
    embedding_dim: 128
    hidden_dim: 256
train:
  epochs: 1
  stop_after_two_steps: True
  learning_rate: 0.01
  max_grad_norm: 5.0