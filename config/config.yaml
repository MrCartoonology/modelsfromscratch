drop_into_debugger_on_error: True
#tokenizer: "bert-base-uncased"
#tokenizer: "Salesforce/codegen-350M-mono"
tokenizer: "Salesforce/codet5-base"
device: "mps"

dataloader:
#  data_dir: "/Users/davidschneider/data/language/books/project_gutenberg"
  # get a shallow copy of the pytorch repo - git clone --depth 1 https://github.com/pytorch/pytorch
  files:
    data_dir: "/Users/davidschneider/code/pytorch/pytorch"
    include_hidden_dirs: False
    include_hidden_files: True
    only_include_specified_extensions: True  
    specified_extensions:
      - ".py"
  max_mb: 2   # debuggin, total size of all files, fast runs
  train_ratio: 0.6
  seq_len: 512
  batch_size: 100
  shuffle: True

model_name: "transformer"
models:
  transformer:
    model_dim: 448
    depth: 4
    num_heads: 4
    ff_hidden_dim: 1024
    freq_base: 10000
  
  basic_rnn:
    embedding_dim: 128
    hidden_dim: 256
optim:
  name: "AdamW"
  learning_rate: 0.0005
  adam_args:
    betas: [0.9, 0.98]
    eps: 1.e-8
    weight_decay: 0.01
train:
  num_epochs: 2
  max_steps: 0
  max_grad_norm: 1.0
  logdir: "logs/{timestamp}"
  savedir: "saved_models/{timestamp}"
  save: True
  save_every_epoch: True
  
eval:
  enable: True
  train_steps_between_evals: 2
  steps_for_eval: 2
  compute_checksum: False
prompt:
  enable: True
  train_steps_between_prompts: 2
  temperature: 0.7
  num_tokens: 35
  max_loss: 33.0