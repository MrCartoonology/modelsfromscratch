drop_into_debugger_on_error: True
drop_into_ipython_on_ctrl_c: True
#tokenizer: "bert-base-uncased"
tokenizer: "Salesforce/codegen-350M-mono"
device: "mps"

dataloader:
#  data_dir: "/Users/davidschneider/data/language/books/project_gutenberg"
  # get a shallow copy of the pytorch repo - git clone --depth 1 https://github.com/pytorch/pytorch
  files:
    data_dir: "/Users/davidschneider/code/pytorch/pytorch"
    include_hidden_dirs: False
    include_hidden_files: True
    only_include_specified_extensions: True  
    specified_extensions:
      - ".py"
  max_mb: 100
  train_ratio: 0.8
  seq_len: 512
  batch_size: 16
  shuffle: True

model_name: "transformer"
models:
  transformer:
    model_dim: 512
    depth: 8
    num_heads: 4
    ff_hidden_dim: 2048
    freq_base: 10000
  
  basic_rnn:
    embedding_dim: 128
    hidden_dim: 256
train:
  epochs: 5
  steps: 0
  learning_rate: 0.0005
  max_grad_norm: 1.0