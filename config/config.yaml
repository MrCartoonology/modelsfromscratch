drop_into_debugger_on_error: True
#tokenizer: "bert-base-uncased"
#tokenizer: "Salesforce/codegen-350M-mono"
tokenizer: "Salesforce/codet5-base"
device: "mps"

dataloader:
#  data_dir: "/Users/davidschneider/data/language/books/project_gutenberg"
  # get a shallow copy of the pytorch repo - git clone --depth 1 https://github.com/pytorch/pytorch
  files:
    data_dir: "/Users/davidschneider/code/pytorch/pytorch"
    include_hidden_dirs: False
    include_hidden_files: True
    only_include_specified_extensions: True  
    specified_extensions:
      - ".py"
  max_mb: 300   # debuggin, total size of all files, fast runs
  train_ratio: 0.8
  seq_len: 512
  batch_size: 80
  shuffle: True

model_name: "transformer"
models:
  transformer:
    model_dim: 512
    depth: 8
    num_heads: 4
    ff_hidden_dim: 2048
    freq_base: 10000
  
  basic_rnn:
    embedding_dim: 128
    hidden_dim: 256
train:
  num_epochs: 1
  max_steps: 0
  learning_rate: 0.0005
  max_grad_norm: 1.0
  logdir: "logs/{timestamp}"
eval:
  enable: False
  train_steps_between_evals: 9
  steps_for_eval: 20
  compute_checksum: True